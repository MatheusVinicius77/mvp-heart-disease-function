{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1c103242",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c60b11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CÉLULA 1: INSTALAÇÃO\n",
        "# ================================================================================\n",
        "%pip install tabm rtdl_num_embeddings optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9de700fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TabM e rtdl_num_embeddings importados com sucesso!\n",
            "Usando dispositivo: cpu\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# CÉLULA 2: IMPORTS E CONFIGURAÇÕES\n",
        "# ================================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import optuna\n",
        "\n",
        "try:\n",
        "    from tabm import TabM\n",
        "    from rtdl_num_embeddings import PiecewiseLinearEmbeddings, LinearReLUEmbeddings\n",
        "    print(\"TabM e rtdl_num_embeddings importados com sucesso!\")\n",
        "except ImportError:\n",
        "    print(\"Bibliotecas não encontradas. Execute a célula de instalação acima e reinicie o kernel se necessário.\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações\n",
        "N_SPLITS = 10\n",
        "RANDOM_STATE = 42\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Usando dispositivo: {DEVICE}\")\n",
        "\n",
        "# URL do dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "\n",
        "# Nomes das colunas\n",
        "columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
        "           'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9e731eb4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape numérico: (297, 5)\n",
            "Shape categórico: (297, 8)\n",
            "Cardinalidades: [2, 4, 2, 3, 2, 3, 4, 3]\n",
            "Distribuição do target: [160 137]\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# CÉLULA 3: CARREGAMENTO E PRÉ-PROCESSAMENTO DE DADOS\n",
        "# ================================================================================\n",
        "def load_and_process_data(url, columns):\n",
        "    # Carregar dados\n",
        "    # O dataset tem valores ausentes marcados com '?'\n",
        "    df = pd.read_csv(url, names=columns, na_values='?')\n",
        "    \n",
        "    # Tratar valores ausentes (remoção simples para este MVP)\n",
        "    df = df.dropna()\n",
        "    \n",
        "    # Converter target para binário (0 = ausência, 1-4 = presença)\n",
        "    df['target'] = df['target'].apply(lambda x: 1 if x > 0 else 0)\n",
        "    \n",
        "    # Separar features numéricas e categóricas\n",
        "    # Numéricas: age, trestbps, chol, thalach, oldpeak\n",
        "    num_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "    # Categóricas: sex, cp, fbs, restecg, exang, slope, ca, thal\n",
        "    cat_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
        "    \n",
        "    # Padronizar features numéricas\n",
        "    scaler = StandardScaler()\n",
        "    df[num_features] = scaler.fit_transform(df[num_features])\n",
        "    \n",
        "    # Codificar categóricas para índices (0 a C-1) para uso com TabM\n",
        "    cat_cardinalities = []\n",
        "    for col in cat_features:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "        cat_cardinalities.append(len(le.classes_))\n",
        "        \n",
        "    X_num = df[num_features].values.astype(np.float32)\n",
        "    X_cat = df[cat_features].values.astype(np.int64)\n",
        "    y = df['target'].values.astype(np.float32).reshape(-1, 1)\n",
        "        \n",
        "    return X_num, X_cat, y, num_features, cat_features, cat_cardinalities\n",
        "\n",
        "X_num, X_cat, y, feature_names_num, feature_names_cat, cat_cardinalities = load_and_process_data(url, columns)\n",
        "\n",
        "print(f\"Shape numérico: {X_num.shape}\")\n",
        "print(f\"Shape categórico: {X_cat.shape}\")\n",
        "print(f\"Cardinalidades: {cat_cardinalities}\")\n",
        "print(f\"Distribuição do target: {np.bincount(y.flatten().astype(int))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c8ffd298",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Realizando Seleção de Features <<<\n",
            "Features originais: 13\n",
            "Removendo: ['sex', 'age', 'trestbps', 'restecg', 'fbs', 'thalach', 'chol']\n",
            "Shape numérico atualizado: (297, 1)\n",
            "Shape categórico atualizado: (297, 5)\n",
            "Features Numéricas restantes: ['oldpeak']\n",
            "Features Categóricas restantes: ['cp', 'exang', 'slope', 'ca', 'thal']\n",
            "Cardinalidades atualizadas: [4, 2, 3, 4, 3]\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# CÉLULA 3.1: SELEÇÃO DE FEATURES (CONFIGURAÇÃO FINAL)\n",
        "# ================================================================================\n",
        "def select_features(X_num, X_cat, feature_names_num, feature_names_cat, cat_cardinalities, features_to_remove):\n",
        "    print(f\"\\n>>> Realizando Seleção de Features <<<\")\n",
        "    print(f\"Features originais: {len(feature_names_num) + len(feature_names_cat)}\")\n",
        "    print(f\"Removendo: {features_to_remove}\")\n",
        "    \n",
        "    # Identificar índices para manter\n",
        "    keep_num_indices = [i for i, f in enumerate(feature_names_num) if f not in features_to_remove]\n",
        "    keep_cat_indices = [i for i, f in enumerate(feature_names_cat) if f not in features_to_remove]\n",
        "    \n",
        "    # Filtrar arrays\n",
        "    X_num_selected = X_num[:, keep_num_indices]\n",
        "    X_cat_selected = X_cat[:, keep_cat_indices]\n",
        "    \n",
        "    # Atualizar nomes\n",
        "    new_feature_names_num = [feature_names_num[i] for i in keep_num_indices]\n",
        "    new_feature_names_cat = [feature_names_cat[i] for i in keep_cat_indices]\n",
        "    \n",
        "    # Atualizar cardinalidades para as categóricas mantidas\n",
        "    new_cat_cardinalities = [cat_cardinalities[i] for i in keep_cat_indices]\n",
        "    \n",
        "    return X_num_selected, X_cat_selected, new_feature_names_num, new_feature_names_cat, new_cat_cardinalities\n",
        "\n",
        "# Lista Final de Features para remover (Mantendo apenas oldpeak, cp, exang, slope, ca, thal)\n",
        "features_to_drop = ['sex', 'age', 'trestbps', 'restecg', 'fbs', 'thalach', 'chol']\n",
        "\n",
        "X_num, X_cat, feature_names_num, feature_names_cat, cat_cardinalities = select_features(\n",
        "    X_num, X_cat, feature_names_num, feature_names_cat, cat_cardinalities, features_to_drop\n",
        ")\n",
        "\n",
        "print(f\"Shape numérico atualizado: {X_num.shape}\")\n",
        "print(f\"Shape categórico atualizado: {X_cat.shape}\")\n",
        "print(f\"Features Numéricas restantes: {feature_names_num}\")\n",
        "print(f\"Features Categóricas restantes: {feature_names_cat}\")\n",
        "print(f\"Cardinalidades atualizadas: {cat_cardinalities}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "6899cf75",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> TESTE DE SMOTE <<<\n",
            "Usando sampling_strategy=1.0 (balanceamento perfeito - conforme literatura)\n",
            "SMOTE: Gerando 23 amostras sintéticas\n",
            "  Classe minoritária: 1 (137 amostras)\n",
            "  Classe majoritária: 0 (160 amostras)\n",
            "  Dataset original: 297 amostras\n",
            "  Dataset com SMOTE: 320 amostras\n",
            "  Nova distribuição: [160 160]\n",
            "\n",
            "Distribuição original: [160 137]\n",
            "Distribuição com SMOTE: [160 160]\n",
            "Amostras sintéticas geradas: 23\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# CÉLULA SMOTE\n",
        "# ================================================================================\n",
        "# Implementação de SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "# para gerar dados sintéticos e melhorar a generalização em folds problemáticos\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def smote_hybrid(X_num, X_cat, y, sampling_strategy=0.5, k_neighbors=5, random_state=42):\n",
        "    \"\"\"\n",
        "    Implementa SMOTE híbrido para dados mistos (numéricos + categóricos).\n",
        "    \n",
        "    Args:\n",
        "        X_num: Array de features numéricas (n_samples, n_num_features)\n",
        "        X_cat: Array de features categóricas (n_samples, n_cat_features)\n",
        "        y: Array de labels (n_samples,)\n",
        "        sampling_strategy: Razão de oversampling (0.5 = 50% da classe majoritária)\n",
        "        k_neighbors: Número de vizinhos para KNN\n",
        "        random_state: Seed para reprodutibilidade\n",
        "        \n",
        "    Returns:\n",
        "        X_num_smote, X_cat_smote, y_smote: Dados aumentados com amostras sintéticas\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Identificar classes\n",
        "    unique_classes = np.unique(y)\n",
        "    if len(unique_classes) != 2:\n",
        "        raise ValueError(\"SMOTE requer classificação binária\")\n",
        "    \n",
        "    # Classe minoritária e majoritária\n",
        "    class_counts = np.bincount(y.astype(int).flatten())\n",
        "    minority_class = np.argmin(class_counts)\n",
        "    majority_class = np.argmax(class_counts)\n",
        "    \n",
        "    minority_mask = (y == minority_class).flatten()\n",
        "    majority_mask = (y == majority_class).flatten()\n",
        "    \n",
        "    X_num_minority = X_num[minority_mask]\n",
        "    X_cat_minority = X_cat[minority_mask]\n",
        "    y_minority = y[minority_mask]\n",
        "    \n",
        "    n_minority = X_num_minority.shape[0]\n",
        "    n_majority = np.sum(majority_mask)\n",
        "    \n",
        "    # Calcular número de amostras sintéticas a gerar\n",
        "    n_synthetic = max(0, int((n_majority * sampling_strategy) - n_minority))\n",
        "    \n",
        "    if n_synthetic == 0:\n",
        "        print(f\"SMOTE: Sem necessidade de oversampling (minoritária: {n_minority}, majoritária: {n_majority})\")\n",
        "        print(f\"  Dica: Use sampling_strategy ≥ {np.ceil((n_minority / n_majority) * 100) / 100} para gerar amostras\")\n",
        "        return X_num, X_cat, y\n",
        "    \n",
        "    print(f\"SMOTE: Gerando {n_synthetic} amostras sintéticas\")\n",
        "    print(f\"  Classe minoritária: {minority_class} ({n_minority} amostras)\")\n",
        "    print(f\"  Classe majoritária: {majority_class} ({n_majority} amostras)\")\n",
        "    \n",
        "    # Normalizar features numéricas para KNN (escala 0-1)\n",
        "    X_num_min_norm = (X_num_minority - X_num_minority.min(axis=0)) / (X_num_minority.max(axis=0) - X_num_minority.min(axis=0) + 1e-8)\n",
        "    \n",
        "    # Encontrar k vizinhos mais próximos (usando apenas features numéricas para distância)\n",
        "    nbrs = NearestNeighbors(n_neighbors=k_neighbors + 1).fit(X_num_min_norm)\n",
        "    distances, indices = nbrs.kneighbors(X_num_min_norm)\n",
        "    \n",
        "    # Gerar amostras sintéticas\n",
        "    X_num_synthetic = []\n",
        "    X_cat_synthetic = []\n",
        "    \n",
        "    for _ in range(n_synthetic):\n",
        "        # Selecionar aleatoriamente uma amostra minoritária\n",
        "        idx_minority = np.random.randint(0, n_minority)\n",
        "        \n",
        "        # Selecionar aleatoriamente um vizinho (excluindo o próprio índice)\n",
        "        neighbor_idx = np.random.choice(indices[idx_minority][1:])\n",
        "        \n",
        "        # Interpolação linear para features numéricas\n",
        "        alpha = np.random.rand(X_num_minority.shape[1])\n",
        "        X_num_new = X_num_minority[idx_minority] + alpha * (X_num_minority[neighbor_idx] - X_num_minority[idx_minority])\n",
        "        \n",
        "        # Para features categóricas: selecionar aleatoriamente entre as duas amostras\n",
        "        X_cat_new = np.where(\n",
        "            np.random.rand(X_cat_minority.shape[1]) > 0.5,\n",
        "            X_cat_minority[idx_minority],\n",
        "            X_cat_minority[neighbor_idx]\n",
        "        )\n",
        "        \n",
        "        X_num_synthetic.append(X_num_new)\n",
        "        X_cat_synthetic.append(X_cat_new)\n",
        "    \n",
        "    # Concatenar dados originais com sintéticos\n",
        "    X_num_smote = np.vstack([X_num, np.array(X_num_synthetic)])\n",
        "    X_cat_smote = np.vstack([X_cat, np.array(X_cat_synthetic)])\n",
        "    y_smote = np.vstack([y, np.full((n_synthetic, 1), minority_class)])\n",
        "    \n",
        "    print(f\"  Dataset original: {X_num.shape[0]} amostras\")\n",
        "    print(f\"  Dataset com SMOTE: {X_num_smote.shape[0]} amostras\")\n",
        "    print(f\"  Nova distribuição: {np.bincount(y_smote.astype(int).flatten())}\")\n",
        "    \n",
        "    return X_num_smote, X_cat_smote, y_smote\n",
        "\n",
        "# Teste com sampling_strategy=1.0 (balanceamento perfeito - recomendado na literatura)\n",
        "print(\"\\n>>> TESTE DE SMOTE <<<\")\n",
        "print(\"Usando sampling_strategy=1.0 (balanceamento perfeito - conforme literatura)\")\n",
        "X_num_test, X_cat_test, y_test = smote_hybrid(X_num, X_cat, y, sampling_strategy=1.0, k_neighbors=5)\n",
        "print(f\"\\nDistribuição original: {np.bincount(y.astype(int).flatten())}\")\n",
        "print(f\"Distribuição com SMOTE: {np.bincount(y_test.astype(int).flatten())}\")\n",
        "print(f\"Amostras sintéticas geradas: {y_test.shape[0] - y.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3184bf8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CÉLULA 4: DEFINIÇÃO DO MODELO E FUNÇÕES DE TREINAMENTO\n",
        "# ================================================================================\n",
        "import rtdl_num_embeddings\n",
        "\n",
        "def create_tabm_model(n_num_features, cat_cardinalities, params, X_train=None, d_out=1):\n",
        "    \"\"\"\n",
        "    Cria o modelo TabM com hiperparâmetros dinâmicos.\n",
        "    Adiciona suporte a Embeddings Numéricos se especificado.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Configuração de Embeddings Numéricos (Opcional mas recomendado)\n",
        "    num_embeddings = None\n",
        "    if params.get('use_embeddings', False) and X_train is not None:\n",
        "        # Usando PiecewiseLinearEmbeddings com cálculo de bins\n",
        "        bins = rtdl_num_embeddings.compute_bins(X_train, n_bins=params['n_bins'])\n",
        "        \n",
        "        num_embeddings = PiecewiseLinearEmbeddings(\n",
        "            bins=bins,\n",
        "            d_embedding=params['d_embedding'],\n",
        "            activation=False, # Geralmente False para TabM\n",
        "            version='B'\n",
        "        )\n",
        "\n",
        "    model = TabM.make(\n",
        "        n_num_features=n_num_features,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        d_out=d_out,\n",
        "        n_blocks=params['n_blocks'],\n",
        "        d_block=params['d_block'],\n",
        "        dropout=params.get('dropout', 0.1),\n",
        "        num_embeddings=num_embeddings\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for X_num_batch, X_cat_batch, y_batch in loader:\n",
        "        X_num_batch = X_num_batch.to(device).float()\n",
        "        X_cat_batch = X_cat_batch.to(device)\n",
        "        y_batch = y_batch.to(device).float()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        y_pred = model(X_num_batch, X_cat_batch)\n",
        "        \n",
        "        # Loss média das k predições independentes\n",
        "        y_target_expanded = y_batch.unsqueeze(1).expand(-1, model.k, -1)\n",
        "        loss = criterion(y_pred.reshape(-1, 1), y_target_expanded.reshape(-1, 1))\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item() * X_num_batch.size(0)\n",
        "        \n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_num_batch, X_cat_batch, y_batch in loader:\n",
        "            X_num_batch = X_num_batch.to(device).float()\n",
        "            X_cat_batch = X_cat_batch.to(device)\n",
        "            \n",
        "            # Ensemble mean na inferência\n",
        "            y_pred = model(X_num_batch, X_cat_batch)\n",
        "            probs = torch.sigmoid(y_pred).mean(dim=1)\n",
        "            \n",
        "            all_preds.append(probs.cpu().numpy())\n",
        "            all_targets.append(y_batch.numpy())\n",
        "            \n",
        "    return np.vstack(all_preds), np.vstack(all_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "8347e2e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-03 01:58:24,742] A new study created in memory with name: no-name-4f41acd3-4115-4e92-9342-a2118e9c0fa9\n",
            "[W 2025-12-03 01:58:24,748] Trial 0 failed with parameters: {'n_blocks': 1, 'd_block': 464, 'lr': 0.00034951114349209103, 'weight_decay': 1.1917886990144126e-05, 'dropout': 0.04386612981947191, 'd_embedding': 8, 'n_bins': 20, 'smote_sampling_strategy': 0.9910453669389785} because of the following error: RuntimeError('expected scalar type Float but found Double').\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/matheus/ifpe/tcc/v4/venv/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/tmp/ipykernel_408719/2788312526.py\", line 65, in objective\n",
            "    train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
            "    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_408719/3141238309.py\", line 46, in train_epoch\n",
            "    y_pred = model(X_num_batch, X_cat_batch)\n",
            "  File \"/home/matheus/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/matheus/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/matheus/ifpe/tcc/v4/venv/lib/python3.13/site-packages/tabm.py\", line 1792, in forward\n",
            "    x_.append(x_num if self.num_module is None else self.num_module(x_num))\n",
            "                                                    ~~~~~~~~~~~~~~~^^^^^^^\n",
            "  File \"/home/matheus/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/matheus/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/matheus/ifpe/tcc/v4/venv/lib/python3.13/site-packages/rtdl_num_embeddings.py\", line 802, in forward\n",
            "    x_ple = self.linear(x_ple)\n",
            "  File \"/home/matheus/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/matheus/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/matheus/ifpe/tcc/v4/venv/lib/python3.13/site-packages/rtdl_num_embeddings.py\", line 209, in forward\n",
            "    x = x @ self.weight\n",
            "        ~~^~~~~~~~~~~~~\n",
            "RuntimeError: expected scalar type Float but found Double\n",
            "[W 2025-12-03 01:58:24,749] Trial 0 failed with value None.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando estudo de otimização com SMOTE (sampling_strategy: 0.86-1.0)...\n",
            "SMOTE: Gerando 13 amostras sintéticas\n",
            "  Classe minoritária: 1 (92 amostras)\n",
            "  Classe majoritária: 0 (106 amostras)\n",
            "  Dataset original: 198 amostras\n",
            "  Dataset com SMOTE: 211 amostras\n",
            "  Nova distribuição: [106 105]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "expected scalar type Float but found Double",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIniciando estudo de otimização com SMOTE (sampling_strategy: 0.86-1.0)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMelhores hiperparâmetros:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m best_params = study.best_params\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/optuna/study/_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/optuna/study/_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/optuna/study/_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/optuna/study/_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     63\u001b[39m best_fold_auc = \u001b[32m0\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m30\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     val_preds, val_targets = evaluate(model, val_loader, DEVICE)\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device)\u001b[39m\n\u001b[32m     43\u001b[39m optimizer.zero_grad()\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m y_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_num_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_cat_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Loss média das k predições independentes\u001b[39;00m\n\u001b[32m     49\u001b[39m y_target_expanded = y_batch.unsqueeze(\u001b[32m1\u001b[39m).expand(-\u001b[32m1\u001b[39m, model.k, -\u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/tabm.py:1792\u001b[39m, in \u001b[36mTabM.forward\u001b[39m\u001b[34m(self, x_num, x_cat)\u001b[39m\n\u001b[32m   1790\u001b[39m x_: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x_num \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1792\u001b[39m     x_.append(x_num \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_num\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x_cat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1794\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cat_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, _INTERNAL_ERROR_MESSAGE\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/rtdl_num_embeddings.py:802\u001b[39m, in \u001b[36mPiecewiseLinearEmbeddings.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    799\u001b[39m x_linear = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.linear0 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.linear0(x)\n\u001b[32m    801\u001b[39m x_ple = \u001b[38;5;28mself\u001b[39m.impl(x)\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m x_ple = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_ple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.activation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    804\u001b[39m     x_ple = \u001b[38;5;28mself\u001b[39m.activation(x_ple)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ifpe/tcc/v4/venv/lib/python3.13/site-packages/rtdl_num_embeddings.py:209\u001b[39m, in \u001b[36m_NLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m x.shape[-(\u001b[38;5;28mself\u001b[39m.weight.ndim - \u001b[32m1\u001b[39m) :] == \u001b[38;5;28mself\u001b[39m.weight.shape[:-\u001b[32m1\u001b[39m]\n\u001b[32m    208\u001b[39m x = x.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m x = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\n\u001b[32m    210\u001b[39m x = x.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mRuntimeError\u001b[39m: expected scalar type Float but found Double"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# CÉLULA 5: OTIMIZAÇÃO DE HIPERPARÂMETROS (OPTUNA) - COM SMOTE\n",
        "# ================================================================================\n",
        "import json\n",
        "import os\n",
        "\n",
        "def objective(trial):\n",
        "    # Espaço de busca sugerido na documentação do TabM\n",
        "    params = {\n",
        "        'n_blocks': trial.suggest_int('n_blocks', 1, 4), \n",
        "        'd_block': trial.suggest_int('d_block', 64, 512, step=16), \n",
        "        'lr': trial.suggest_float('lr', 1e-4, 5e-3, log=True),\n",
        "        'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-1, log=True),\n",
        "        'dropout': trial.suggest_float('dropout', 0.0, 0.5),\n",
        "        \n",
        "        # Configurações de Embeddings\n",
        "        'use_embeddings': True,\n",
        "        'd_embedding': trial.suggest_int('d_embedding', 8, 32, step=4),\n",
        "        'n_bins': trial.suggest_int('n_bins', 2, 64),\n",
        "        \n",
        "        # Configurações de SMOTE (conforme literatura)\n",
        "        'use_smote': True,\n",
        "        'smote_sampling_strategy': trial.suggest_float('smote_sampling_strategy', 0.86, 1.0)\n",
        "    }\n",
        "    \n",
        "    # Validação cruzada interna (3 folds para ser rápido)\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
        "    scores = []\n",
        "    \n",
        "    for train_idx, val_idx in skf.split(X_num, y):\n",
        "        # Split simples\n",
        "        X_num_train, X_num_val = X_num[train_idx], X_num[val_idx]\n",
        "        X_cat_train, X_cat_val = X_cat[train_idx], X_cat[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "        \n",
        "        # Aplicar SMOTE ao conjunto de treinamento\n",
        "        if params['use_smote']:\n",
        "            X_num_train, X_cat_train, y_train = smote_hybrid(\n",
        "                X_num_train, X_cat_train, y_train,\n",
        "                sampling_strategy=params['smote_sampling_strategy'],\n",
        "                k_neighbors=5,\n",
        "                random_state=RANDOM_STATE\n",
        "            )\n",
        "        \n",
        "        train_ds = TensorDataset(torch.tensor(X_num_train), torch.tensor(X_cat_train), torch.tensor(y_train))\n",
        "        val_ds = TensorDataset(torch.tensor(X_num_val), torch.tensor(X_cat_val), torch.tensor(y_val))\n",
        "        \n",
        "        train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "        val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "        \n",
        "        # Passar X_num_train para calcular os bins dos embeddings\n",
        "        model = create_tabm_model(\n",
        "            len(feature_names_num), \n",
        "            cat_cardinalities, \n",
        "            params, \n",
        "            X_train=torch.tensor(X_num_train)\n",
        "        ).to(DEVICE)\n",
        "        \n",
        "        optimizer = optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "        # Treino curto (30 épocas)\n",
        "        best_fold_auc = 0\n",
        "        for epoch in range(30):\n",
        "            train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
        "            val_preds, val_targets = evaluate(model, val_loader, DEVICE)\n",
        "            try:\n",
        "                auc = roc_auc_score(val_targets, val_preds)\n",
        "            except:\n",
        "                auc = 0.5\n",
        "            best_fold_auc = max(best_fold_auc, auc)\n",
        "            \n",
        "            # Pruning do Optuna\n",
        "            trial.report(best_fold_auc, epoch)\n",
        "            if trial.should_prune():\n",
        "                raise optuna.exceptions.TrialPruned()\n",
        "                \n",
        "        scores.append(best_fold_auc)\n",
        "    \n",
        "    return np.mean(scores)\n",
        "\n",
        "print(\"Iniciando estudo de otimização com SMOTE (sampling_strategy: 0.86-1.0)...\")\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100) \n",
        "\n",
        "print(\"\\nMelhores hiperparâmetros:\")\n",
        "best_params = study.best_params\n",
        "best_params['use_embeddings'] = True \n",
        "best_params['use_smote'] = True\n",
        "print(best_params)\n",
        "\n",
        "# Salvar em JSON\n",
        "output_dir = '/home/matheus/ifpe/tcc/v4/mvp-heart-disease-function/TabM'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "json_path = os.path.join(output_dir, 'best_params_tabm.json')\n",
        "\n",
        "with open(json_path, 'w') as f:\n",
        "    json.dump(best_params, f, indent=4)\n",
        "    \n",
        "print(f\"Melhores parâmetros salvos em: {json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc632c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execute a célula de otimização primeiro!\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# CÉLULA 6: EXECUÇÃO DO EXPERIMENTO FINAL (COM SMOTE E THRESHOLD OTIMIZADO)\n",
        "# ================================================================================\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def run_final_experiment(params):\n",
        "    print(\"\\n>>> Rodando Experimento Final com Melhores Parâmetros (com SMOTE) <<<\")\n",
        "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    \n",
        "    fold_auc_results = []\n",
        "    fold_acc_results = []\n",
        "    fold_opt_acc_results = [] # Acurácia com threshold otimizado\n",
        "    fold_smote_info = []  # Informações sobre SMOTE por fold\n",
        "    \n",
        "    tprs = []\n",
        "    aucs = []\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    \n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_num, y)):\n",
        "        print(f\"\\nFold {fold+1}/{N_SPLITS}\")\n",
        "        \n",
        "        X_num_train, X_num_val = X_num[train_idx], X_num[val_idx]\n",
        "        X_cat_train, X_cat_val = X_cat[train_idx], X_cat[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "        \n",
        "        # Aplicar SMOTE ao conjunto de treinamento\n",
        "        if params.get('use_smote', False):\n",
        "            print(f\"  Aplicando SMOTE...\")\n",
        "            X_num_train_orig_size = X_num_train.shape[0]\n",
        "            X_num_train, X_cat_train, y_train = smote_hybrid(\n",
        "                X_num_train, X_cat_train, y_train,\n",
        "                sampling_strategy=params.get('smote_sampling_strategy', 0.8),\n",
        "                k_neighbors=5,\n",
        "                random_state=RANDOM_STATE + fold  # Seed diferente por fold\n",
        "            )\n",
        "            fold_smote_info.append({\n",
        "                'fold': fold + 1,\n",
        "                'original_size': X_num_train_orig_size,\n",
        "                'smote_size': X_num_train.shape[0],\n",
        "                'increase': X_num_train.shape[0] - X_num_train_orig_size\n",
        "            })\n",
        "        \n",
        "        train_loader = DataLoader(TensorDataset(torch.tensor(X_num_train), torch.tensor(X_cat_train), torch.tensor(y_train)), batch_size=256, shuffle=True)\n",
        "        val_loader = DataLoader(TensorDataset(torch.tensor(X_num_val), torch.tensor(X_cat_val), torch.tensor(y_val)), batch_size=256, shuffle=False)\n",
        "        \n",
        "        # Passar X_num_train para embeddings\n",
        "        model = create_tabm_model(\n",
        "            len(feature_names_num), \n",
        "            cat_cardinalities, \n",
        "            params, \n",
        "            X_train=torch.tensor(X_num_train)\n",
        "        ).to(DEVICE)\n",
        "        \n",
        "        optimizer = optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "        best_val_auc = 0\n",
        "        best_epoch_preds = None\n",
        "        best_epoch_targets = None\n",
        "        patience = 15 \n",
        "        no_improve = 0\n",
        "        \n",
        "        for epoch in range(100):\n",
        "            train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
        "            val_preds, val_targets = evaluate(model, val_loader, DEVICE)\n",
        "            \n",
        "            try:\n",
        "                val_auc = roc_auc_score(val_targets, val_preds)\n",
        "            except:\n",
        "                val_auc = 0.5\n",
        "            \n",
        "            if val_auc > best_val_auc:\n",
        "                best_val_auc = val_auc\n",
        "                best_epoch_preds = val_preds\n",
        "                best_epoch_targets = val_targets\n",
        "                no_improve = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "                \n",
        "            if no_improve >= patience:\n",
        "                break\n",
        "        \n",
        "        # Calcular métricas finais para o melhor modelo do fold\n",
        "        if best_epoch_preds is not None:\n",
        "            # 1. ROC e Threshold Otimizado (Youden's J)\n",
        "            fpr, tpr, thresholds = roc_curve(best_epoch_targets, best_epoch_preds)\n",
        "            J = tpr - fpr\n",
        "            ix = np.argmax(J)\n",
        "            best_thresh = thresholds[ix]\n",
        "            \n",
        "            # Acurácias\n",
        "            acc_default = accuracy_score(best_epoch_targets, (best_epoch_preds > 0.5).astype(int))\n",
        "            acc_opt = accuracy_score(best_epoch_targets, (best_epoch_preds > best_thresh).astype(int))\n",
        "            \n",
        "            fold_auc_results.append(best_val_auc)\n",
        "            fold_acc_results.append(acc_default)\n",
        "            fold_opt_acc_results.append(acc_opt)\n",
        "            \n",
        "            # Plotting\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            aucs.append(roc_auc)\n",
        "            interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "            interp_tpr[0] = 0.0\n",
        "            tprs.append(interp_tpr)\n",
        "            ax.plot(fpr, tpr, lw=1, alpha=0.3)\n",
        "            \n",
        "            print(f\"  AUC: {best_val_auc:.4f} | Acc (0.5): {acc_default:.4f} | Acc (Opt {best_thresh:.2f}): {acc_opt:.4f}\")\n",
        "\n",
        "    # Plot Média\n",
        "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance')\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    std_auc = np.std(aucs)\n",
        "    ax.plot(mean_fpr, mean_tpr, color='b', label=r'Mean ROC (AUC = %0.4f $\\pm$ %0.4f)' % (mean_auc, std_auc), lw=2)\n",
        "    \n",
        "    std_tpr = np.std(tprs, axis=0)\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2)\n",
        "    \n",
        "    ax.set(title=\"ROC - TabM Otimizado com SMOTE\", xlabel='FPR', ylabel='TPR')\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n=== RESULTADOS FINAIS ===\")\n",
        "    print(f\"Média AUC-ROC: {np.mean(fold_auc_results):.4f} (+/- {np.std(fold_auc_results):.4f})\")\n",
        "    print(f\"Média Accuracy (Threshold 0.5): {np.mean(fold_acc_results):.4f} (+/- {np.std(fold_acc_results):.4f})\")\n",
        "    print(f\"Média Accuracy (Threshold Otimizado): {np.mean(fold_opt_acc_results):.4f} (+/- {np.std(fold_opt_acc_results):.4f})\")\n",
        "    \n",
        "    if fold_smote_info:\n",
        "        print(f\"\\n=== INFORMAÇÕES DE SMOTE ===\")\n",
        "        for info in fold_smote_info:\n",
        "            print(f\"Fold {info['fold']}: {info['original_size']} → {info['smote_size']} (+{info['increase']} sintéticas)\")\n",
        "\n",
        "if 'best_params' in globals():\n",
        "    run_final_experiment(best_params)\n",
        "else:\n",
        "    print(\"Execute a célula de otimização primeiro!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13761c51",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CÉLULA 7: ESTIMATIVA DE INCERTEZA E IMPORTÂNCIA DE FEATURES\n",
        "# ================================================================================\n",
        "\n",
        "def analyze_uncertainty_and_features(model, X_num_val, X_cat_val, y_val, feature_names):\n",
        "    \"\"\"\n",
        "    Analisa a incerteza das predições e a importância das features via permutação.\n",
        "    \"\"\"\n",
        "    print(\"\\n>>> Análise de Incerteza e Features <<<\")\n",
        "    model.eval()\n",
        "    \n",
        "    # 1. Incerteza (Desvio Padrão do Ensemble)\n",
        "    X_num_t = torch.tensor(X_num_val).to(DEVICE)\n",
        "    X_cat_t = torch.tensor(X_cat_val).to(DEVICE)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Shape: (batch, k, 1)\n",
        "        y_pred_raw = model(X_num_t, X_cat_t)\n",
        "        probs_ensemble = torch.sigmoid(y_pred_raw).squeeze(-1).cpu().numpy() # (batch, k)\n",
        "        \n",
        "    # Média e Desvio Padrão das probabilidades\n",
        "    mean_probs = probs_ensemble.mean(axis=1)\n",
        "    std_probs = probs_ensemble.std(axis=1)\n",
        "    \n",
        "    # Plotar relação Probabilidade x Incerteza\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(mean_probs, std_probs, alpha=0.5, c=y_val, cmap='coolwarm')\n",
        "    plt.colorbar(label='Target Real (0=Azul, 1=Vermelho)')\n",
        "    plt.title(\"Incerteza (Std Dev) vs Probabilidade Média\")\n",
        "    plt.xlabel(\"Probabilidade Média Predita\")\n",
        "    plt.ylabel(\"Incerteza (Desvio Padrão entre Ensemble)\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Incerteza média: {std_probs.mean():.4f}\")\n",
        "    print(f\"Exemplo de alta incerteza (Index {np.argmax(std_probs)}): Prob={mean_probs[np.argmax(std_probs)]:.2f}, Std={std_probs.max():.2f}\")\n",
        "    \n",
        "    # 2. Importância de Features (Permutation Importance)\n",
        "    print(\"\\nCalculando Importância de Features (Permutação)...\")\n",
        "    baseline_auc = roc_auc_score(y_val, mean_probs)\n",
        "    importances = {}\n",
        "    \n",
        "    # Features Numéricas\n",
        "    for i, name in enumerate(feature_names[:len(feature_names_num)]): # Assumindo ordem numéricas primeiro\n",
        "        X_num_permuted = X_num_val.copy()\n",
        "        np.random.shuffle(X_num_permuted[:, i])\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            y_p = model(torch.tensor(X_num_permuted).to(DEVICE), X_cat_t)\n",
        "            prob_perm = torch.sigmoid(y_p).mean(dim=1).squeeze().cpu().numpy()\n",
        "            \n",
        "        perm_auc = roc_auc_score(y_val, prob_perm)\n",
        "        importances[name] = baseline_auc - perm_auc\n",
        "        \n",
        "    # Features Categóricas\n",
        "    for i, name in enumerate(feature_names[len(feature_names_num):]):\n",
        "        X_cat_permuted = X_cat_val.copy()\n",
        "        np.random.shuffle(X_cat_permuted[:, i])\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            y_p = model(X_num_t, torch.tensor(X_cat_permuted).to(DEVICE))\n",
        "            prob_perm = torch.sigmoid(y_p).mean(dim=1).squeeze().cpu().numpy()\n",
        "        \n",
        "        perm_auc = roc_auc_score(y_val, prob_perm)\n",
        "        importances[name] = baseline_auc - perm_auc\n",
        "        \n",
        "    # Plotar Importâncias\n",
        "    sorted_feats = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
        "    names, vals = zip(*sorted_feats)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(names, vals)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.title(\"Feature Importance (Queda no AUC após permutação)\")\n",
        "    plt.xlabel(\"Queda no AUC\")\n",
        "    plt.show()\n",
        "\n",
        "# Executar análise usando o último modelo treinado e dados de validação do último fold\n",
        "# Precisamos recuperar os dados do último fold do loop anterior\n",
        "# Como run_final_experiment é uma função, não temos acesso direto às variáveis locais.\n",
        "# Vamos rodar uma rápida inferência em um split de validação para demonstrar.\n",
        "\n",
        "print(\"Executando análise no último fold...\")\n",
        "# Recriar split do último fold manualmente para análise\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "splits = list(skf.split(X_num, y))\n",
        "last_train_idx, last_val_idx = splits[-1]\n",
        "\n",
        "X_num_val_last = X_num[last_val_idx]\n",
        "X_cat_val_last = X_cat[last_val_idx]\n",
        "y_val_last = y[last_val_idx]\n",
        "\n",
        "# Precisamos treinar o modelo neste fold para garantir que 'model' existe e está treinado\n",
        "# Como 'model' é local em run_final_experiment, vamos treinar um rápido aqui ou modificar a função anterior.\n",
        "# Para simplificar e não rodar tudo de novo, vou treinar um modelo rápido com os best_params neste split.\n",
        "\n",
        "if 'best_params' in globals():\n",
        "    final_model = create_tabm_model(len(feature_names_num), cat_cardinalities, best_params, X_train=torch.tensor(X_num[last_train_idx])).to(DEVICE)\n",
        "    # Treino rápido para ter pesos não-aleatórios\n",
        "    opt = optim.AdamW(final_model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
        "    crit = nn.BCEWithLogitsLoss()\n",
        "    loader = DataLoader(TensorDataset(torch.tensor(X_num[last_train_idx]), torch.tensor(X_cat[last_train_idx]), torch.tensor(y[last_train_idx])), batch_size=256, shuffle=True)\n",
        "    \n",
        "    print(\"Treinando modelo para análise (pode levar alguns segundos)...\")\n",
        "    for _ in range(50): # 50 épocas suficiente para análise\n",
        "        train_epoch(final_model, loader, opt, crit, DEVICE)\n",
        "        \n",
        "    all_features = feature_names_num + feature_names_cat\n",
        "    analyze_uncertainty_and_features(final_model, X_num_val_last, X_cat_val_last, y_val_last, all_features)\n",
        "else:\n",
        "    print(\"Execute a otimização primeiro.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
